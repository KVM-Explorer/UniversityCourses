---
title: textCNN文本二分类
toc: true
date: 2021-11-23 09:59:00
---

## 1. textCNN 数学推导



## 2. textCNN编程实现

### 2.1 基本网络结构图

### 2.2 基本神经网络层级实现

#### 2.2.1 CNN前向传播

#### 2.2.2 CNN反向传播

#### 2.2.3 Relu 前向传播

#### 2.2.4 Relu 反向传播

#### 2.2.5 pool前向传播

#### 2.2.6 pool反向传播

#### 2.2.7 nn 前向传播

#### 2.2.8 nn 反向传播

#### 2.2.9 softmax 前向传播

#### 2.2.10 softmax 反响传播

#### 2.2.9 损失函数定义

### 2.5 参数调整优化

- 归一化输入数据

- CNN
  - 含有padding
  - 不同步长
- 激活函数
  - relu 
  - sigmod
- pooling
  - max_pooling
  - average_pooling
  - k-gram
- 损失函数
  - 交叉熵
  - 均方差损失
- 训练
  - 批样本batch
  - 单样本

### 2.6 评价展示

- AUC值
- ROC曲线
- 实际效果展示

### 2.7 定义数据接口

假设对于一段文本  I like apple ，我们设计其词向量为4维我们有如下的数据输入

```
I 		-> [1,0,0,0],
like	-> [0,1,0,1],
apple	-> [0,1,1,0],
```

如上所示构成一个文本向量矩阵作为单样本的输入

当面临多样本构成的数据集输入时，我们约定在以三维矩阵进行存储，具体来说，更高一维度代表序号，其他同上

```
eg. I like |  your son
[	
	[	[0,1,0,1]
		[1,0,1,1]],
	[	[1,0,1,1]
		[1,1,2,3]z],
]
```

## 3. 笔记

![在这里插入图片描述](textCNN文本二分类/20200719000556866.png)

测试集上准确率是89%

## 4.日志

随机生成30组数据但是无法拟合，错误率一直在

出现了负损失？？

### 4.1 异常收敛

出现了收敛，但是对于结果来说分类异常

```
softmax:  [0.60842953 0.39157047]
softmax:  [0.60842953 0.39157047]
softmax:  [0.60842953 0.39157047]
softmax:  [0.60842953 0.39157047]
softmax:  [0.60842953 0.39157047]
....
softmax:  [0.5907397 0.4092603]
softmax:  [0.5907397 0.4092603]
softmax:  [0.5907397 0.4092603]
softmax:  [0.5907397 0.4092603]
```

最终训练的趋势变为了均匀分布

![image-20211128101925244](textCNN文本二分类/image-20211128101925244.png)

#### 推测1 测试数据

可能和输入数据有关输入数据严格遵守均值为0的随机分布

尝试使用自定义数据进行测试

```
[[[	[0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1]],

    [[0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1]],

    [[0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1],
    [0.1]],

    [[0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8]],

    [[0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8]],

    [[0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8],
    [0.8]],

      [[0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2]],

      [[0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2]],

      [[0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2]]],
       # ================================
     [[[0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8]],

      [[0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8]],

      [[0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8],
       [0.8]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]]],
       #===========================================
     [[[0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3]],

      [[0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3]],

      [[0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3],
       [0.3]],

      [[0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2],
       [0.2]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]],

      [[0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1],
       [0.1]],

      [[0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6]],

      [[0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6]],

      [[0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6],
       [0.6]]]]
```

实验显示对结果无明显影响推测可能和反向传播有关

#### 推测2 反向传播

在去除多个卷积层保留最基本的简单结构下仍然出现了异常收敛的问题初步断定为全连接层或卷积层的反响传播或者误差计算的问题排除卷积融合的问题





