# Ababoost 分类器

## 简介

我们知道boost系列的分类算法是一种集成算法，就是把多个弱分类器集结在一起构建一个强分类器。所以，其本质上是由这些弱分类器决定，即弱分类是线性分类器则boost实现的分类器是线性分类器，反之则为非线性分类器。而我们的今天的主角——Adaboost在这其中以关注错误样本和性能优秀的弱分类为主要特征。

以python手动实现Adaboost算法,并封装成类

## 算法推导

 对于多元分类，我们可以理解为二分类的扩展即任何一个多元分类的问题我们总可以分解为多个二分类问题，所以我们在此首先讨论Adaboost的二分类问题。

### 二元分类

为实现上述Adaboost的特征我们采用以下方式实现：

1. 为实现对错误样本的关注我们赋予样本不同的权值以支持并记为$W_i$
2. 为实现对错误程度的量化我们记错误率为E
3. 对分类器赋予不同的权值以支持对表现优秀的分类的关注并记为$D_i$
4. 弱分类器的联合调用，我们采取加权的方式进行整合

对于每个弱分类器我们我们怎样实现上述过程呢？首先可以确定的是输入和输出，输入带权样本集X及对应标签Y，输出错误率E和输出当前弱分类器的预测结果
$$
E = P(X\neq Y) \\
$$


 

### 多分类

在构建完单个Adaboost分类器后，对于多分类问题，我们可以有以下两种思路：

1. 每次分类时，我们可以提取一个类别作为一类，将其余类别作为另一类进行二分类。然后，如果不能确定结果即当前分类结果为剩余类的并集，我们继续对该对象用剩余重复上述过程直到分出结果，总共需要构建$n-1$个分类器。
2. 我们可以针对所有已知类别两两匹配构建$ \frac{n(n-1)}{2}$ 个Adaboost分类器，在对目标经过所有分类器判定后，统计分类结果中出现频率最高的即为最终的分类结果。

对于第一种分类方式而言，其构建的分类器数目少因而执行速度快，但是精度较低。对于第二种分类方式，其构建的分类器数目多执行速度慢，但精度较高。

## 伪代码实现

由Adaboost算法本身我们可知，我们可以以任意一种弱分类器算法构建我们的强分类器，在此基础上我们这次选择了单层决策树作为我们的弱分类器算法。

### Adaboost算法



### 单层决策树算法



## 使用需求

- numpy 库


## 使用

### 测试

main.py 中预留了单元测试函数，可用于简单测试adaboost二元分类和adaboost多元分类

### 二元分类

```python
import adaboost
```
1. 实例化adaboost对象，可通过输入额外的参数对ababoost构造进行自定义
2. 创建训练集和标签集,训练集以二维矩阵保存,每行代表一个样本，每列对应一个维度的特征
3. 使用对象的**train**函数进行训练，并输入对应的数据集和标签
4. 训练后，你可以调用对象的**predict** 函数对自己的数据集进行预测

### 多元分类

