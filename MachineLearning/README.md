# Ababoost 分类器

## 简介

我们知道boost系列的分类算法是一种集成算法，就是把多个弱分类器集结在一起构建一个强分类器。所以，其本质上是由这些弱分类器决定，即弱分类是线性分类器则boost实现的分类器是线性分类器，反之则为非线性分类器。而我们的今天的主角——Adaboost在这其中以关注错误样本和性能优秀的弱分类为主要特征。

以python手动实现Adaboost算法,并封装成类

## 算法推导

 对于多元分类，我们可以理解为二分类的扩展即任何一个多元分类的问题我们总可以分解为多个二分类问题，所以我们在此首先讨论Adaboost的二分类问题。

### 二元分类

为实现上述Adaboost的特征我从以下几点考虑实现：

1. 为实现对错误样本的关注我们赋予样本不同的权值以支持并记为$W_i$
2. 为实现对错误程度的量化我们记错误率为$E$
3. 对第k个分类器$G_k$赋予权值以支持对表现优秀的分类的关注并记权值为$D_i$
4. 弱分类器的联合调用，我们采取加权求和的方式进行整合

要真正理解上述问题我们首先要从Adaboost损失优化函数说起

由于Adaboost是一个个弱分类器逐轮训练并加权的强分类器对于前k-1轮产生的弱分类器我们有
$$
f_{k-1}(x) = \Sigma^{k-1}_{1} D_iG_i(x)
$$
当第k轮产生弱分类器$G_k$ 我们有Adaboost强分类器公式为
$$
f_k(x) = \Sigma^{k}_{1}D_iG_i(x)
$$

由此我们可以得到Adaboost分类器的更新公式
$$
f_k(x)  =  f_{k-1}(x) + D_kG_k(x)
$$


对于Adaboost而言，我们有其指数形式的损失函数定义为
$$
loss = \Sigma^{n}_{i=1} e^{-y_i f_k(x)}
$$


其中，$y_i$代表标签值，$f_k(x)$代表我们在第k轮强分类器下的预测值， 在此情况下当两者结果相同指数部分<0我们获得一个较小的损失,,当两者结果相悖指数部分>0 ,即损失>1我们获得较大的损失 ，进而我们的问题转化为一个最优化问题，即
$$
\begin{align}

loss_{min} &= arg \ min_{D,G}f(x) \\

&= arg\ min_{D,G} \Sigma e^{-y_if_k(x)}  \\

&= arg\ min_{D,G} \Sigma e^{-y_i(f_{k-1}(x)+D_kG_k(x)} \\
\end{align}
$$
其中对于$e^{-y_if_k-1(x)}$ 由于这部分仅和迭代有关和我们待求参数无关所以我们可以提取出来记为$t_i$ ，所以现在公式变成了
$$
\begin{align}
loss_{min} &= arg \min_{D,G} \Sigma^{n}_{i=1} t_i e^{-D_kG_k(x)}
\end{align}
$$
由于是一个二分类问题我们有标签集{1,-1}，进而我们对于G(x)可将上公式变换为
$$
\begin{align}
\Sigma^n_{i=1} t_ie^{-D_kG_k(x)} &= \Sigma_{G(x_i)= y_i}t_ie^{-D_k} + \Sigma_{G(x_i)\neq y_i}t_i e^{D_k}	\\
& = e^{-D_k}\Sigma_{G_k(x)=y_i} t_i + e^{-D_k} \Sigma_{G_k\neq y_i}t_i - e^{-D_k}\Sigma_{G_k(x)\neq y_i} + e^{D_k} \Sigma_{G_k(x)=y_i} t_i \\
& = (e^{D_k} - e^{-D_k})\Sigma^{n}_{i=1} {t_{i}I(G_k(x)\neq y_i)} + e^{-D_k}\Sigma^{N}_{i=1}{t_i} \\
\end{align}
$$


进而我们可得第k轮训练的弱分类器$G_k(x)$的优化表达式
$$
G_k(x) = arg \ min \Sigma^{n}_{i=1}{t_iI(G_k(x)\neq y_i)}
$$
我们想要对误差部分最小对所以对(12)式求导可得
$$
\\
(e^{D_k} +e^{-D_k})\Sigma^n_{i=1} {t_i I(G_k(x)\neq y_i)} - e^{-D_k} \Sigma t_i = 0 \\
$$

得到关于$D_k$方程
$$
(e^{D_k} +e ^{-D_k}) E_k - e^{-D_k} = 0 \\

D_k = \frac{1}{2} log\frac{1-E_k}{E_k}
其中，E_k = \frac{\Sigma^{n}_{i=1} t_i I(G_k(x)\neq y_i)}{\Sigma^{n}_{i=1} t_i}  \\
$$

对于每个弱分类器我们又该如何进行具体的优化过程呢？首先对于第k个弱分类器$G_k$我们可以确定的是输入和输出，输入带权样本集X及对应标签集Y，输出错误率$E_k$和输出当前弱分类器的预测结果I,因而对于错误率我们有
$$
E_k=P(G_k(x)\neq y) = \Sigma^{n}_{i=1}{W_i*I(G_k(x)\neq y)} \\
$$

其中，$W_i$ 表示第i个样本的权重，I表示当前分类器的预测结果

进一步Adaboost针对不同的弱分类器赋予不同的权重，在此我们可以这样理解，Adaboost算法希望表现效果更好的弱分类器在最终总结果中占据的话语权更多，而表现更差的弱分类器所占权重相对较小，这也非常符合我们的直觉，进一步根据其损失函数的优化过程我们可以得到对于第k个弱分类器它所占有的权重为
$$
D_k = \frac{1}{2}log\frac{1-E_k}{E_k}
$$
接下来在当前状态下，对于下一个弱分类器而言我们如何计算他的输入带权样本对应的权重呢？

$$
W_{(k+1)i} = \frac{W_{ki}}{Z_k}e^{-D_k y_i G_k(x_i)} 
其中，Z_k = \Sigma^{n}_{i=1}W_{ki}e^{-D y_iG_k(x_i)}
$$



最后，我们怎样使用多个弱分类器构建我们的强分类呢？
$$
f(x) = \Sigma^m_{k=1} D_kG_k(x)
$$


### 多分类

在构建完单个Adaboost分类器后，对于多分类问题，我们可以有以下两种思路：

1. 每次分类时，我们可以提取一个类别作为一类，将其余类别作为另一类进行二分类。然后，如果不能确定结果即当前分类结果为剩余类的并集，我们继续对该对象用剩余重复上述过程直到分出结果，总共需要构建$n-1$个分类器。
2. 我们可以针对所有已知类别两两匹配构建$ \frac{n(n-1)}{2}$ 个Adaboost分类器，在对目标经过所有分类器判定后，统计分类结果中出现频率最高的即为最终的分类结果。

对于第一种分类方式而言，其构建的分类器数目少因而执行速度快，但是精度较低。对于第二种分类方式，其构建的分类器数目多执行速度慢，但精度较高。

## 伪代码实现

由Adaboost算法本身我们可知，我们可以以任意一种弱分类器算法构建我们的强分类器，在此基础上我们这次选择了单层决策树作为我们的弱分类器算法。

### Adaboost算法

输入为样本$\{ (x_1,y_1) (x_2,y_2) (x_3,y_3) ...(x_n,y_n)\}$

1. 初始化训练样本的权重为 $W_{1i} = 1/n$

2. for  k = 1,2,3...,m 构造的弱分类器
	1. 对于第i个弱分类器我们使用权重为$W_{k}$的样本进行训练
	
	2. 计算$G_k$ 的分类误差率
	   $$
	   E_k=P(G_k(x)\neq y) = \Sigma^{n}_{i=1}{W_i*I(G_k(x)\neq y)}
	   $$
	
	3. 计算弱分类在强分类器的权重系数
	   $$
	   D_k = \frac{1}{2}log\frac{1-E_k}{E_k}
	   $$
	
	4. 根据样本在当前分类情况下的结果更新各样本所应该具有的权值
	   $$
	   W_{(k+1)i} = \frac{W_{ki}}{Z_k}e^{-D_k y_i G_k(x_i)} 
	   其中，Z_k = \Sigma^{n}_{i=1}W_{ki}e^{-D y_iG_k(x_i)}
	   $$
	
3.  构建最终的强分类器
   $$
   f(x) = \Sigma^{m}_{k=1} D_k G_k(x)
   $$
   

### 单层决策树算法

输入为样本$\{ (x_1,y_1),(x_2,y_2),...,(x_n,y_n)\} $  及其对应的权重W

for 枚举各个维度代表的特征分量
	获得当前特征分量的最大最小值
	根据两最值构建划分区间并根据参数设置划分步长
	for 根据步长枚举当前阈值threshold
		for 枚举第一类别和和threshold的关系
            if 特征大于threshold 
                根据threshold和当前类别关系判断类别
            else 
                根据threshold和当前类别关系判断类别
            计算当前划分的正确率= $\Sigma $样本权重×正确样本
            if 优于最佳值
                保存当前的阈值，维度和预测结果



## 使用需求

- numpy 库 CPU 运算


## 使用

### 测试

main.py 中预留了单元测试函数，可用于简单测试adaboost二元分类和adaboost多元分类

### 二元分类

```python
import adaboost
```
1. 实例化adaboost对象，可通过输入额外的参数对ababoost构造进行自定义
2. 创建训练集和标签集,训练集以二维矩阵保存,每行代表一个样本，每列对应一个维度的特征
3. 使用对象的**train**函数进行训练，并输入对应的数据集和标签
4. 训练后，你可以调用对象的**predict** 函数对自己的数据集进行预测

### 多元分类

## Todo

- 更新库采用GPU库加速运算
  - cupy
  - MXNET



